---
title: "Interactive Dashboard"
output: 
  flexdashboard::flex_dashboard:
    theme:
      version: 4
      bg: "#101010"
      fg: "#FDF7F7" 
      primary: "#ED79F9"
      navbar-bg: "#3ADAC6"
      base_font: 
    orientation: columns
    vertical_layout: fill
runtime: shiny
css: "Styles.css"  
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, ggplot2, gganimate, flexdashboard, mosaic, moderndive, effectsize, modelr, rsample, car,ggridges,plotly,broom,shiny)





consumption_date <- read.csv("~/Banana_project/consumption_date.csv")
consumption_traffic <-  read_csv("~/Banana_project/consumption_traffic.csv")
Consumption_Log <- read_csv("~/Banana_project/consumption_log.csv")
bananas_thrown_away <- read_csv("~/Banana_project/bananas_thrown_away.csv")
by_time <- consumption_traffic %>% 
  group_by(time_period) %>% 
  summarize(
    total_bananas = sum(bananas_consumed, na.rm = TRUE), 
    avg_bananas = mean(bananas_consumed, na.rm = TRUE)   
  )

by_time_long <- by_time %>% 
  pivot_longer(cols = c(total_bananas, avg_bananas), 
               names_to = "metric", 
               values_to = "value")




```

## Dashboard {.tabset}

### Homepage: The Entire Story 
*Welcome to the project dashboard for banana consumption optimization! This dashboard is designed to document the process, from data collection to insights, visualizations, and actionable outcomes.*

#### Table of Contents:
- [Quick summary of my entire project](#summary)
- [Core Problem and Secondary Questions](#problem-statement)
- [Key Metrics and Visualizations](#key-metrics)
- [Database Schema and Design](#database-schema)
- [Sample Period and Analytical Process](#sample-period)

---

### My initial thought and summary of the whole project {#summary}
*I initially planned to use SQL to keep track of everything and make sure tht it worked alright because I was learning it at the time. However, Google Sheets proved to be much easier and more accessible so I tracked most of my bananna consumption inside of that *


#### Steps:
1. Define the core problem statement.
2. Identify key factors influencing banana consumption.

---

### Core Problem Statement {#problem-statement}
#### Objective:
- Structure and store data efficiently for analysis in **R** and have enough data to perform with statistical tests as well as **machine learning models**
- Ultimately I decided to built this dashboard with **Flexdashobard** as well the **shiny**



#### Main Question:
> **What is the optimal number of bananas to take each day to ensure no bananas are wasted while also meeting daily consumption needs?**

#### Secondary Questions:
- What factors (e.g., day of the week, crowd levels, restock schedules, ripeness) influence banana consumption patterns?
- How does ripeness affect the likelihood of consumption or waste?
- Are there specific times or days when taking bananas is more efficient to minimize waste and hunger?
- Can predictive modeling improve restocking and consumption decisions?

---

### Key Metrics and Visualizations {#key-metrics}
#### Metrics:
- **Ripeness**
- **Date**
- **Is_weekend**
- **Consumption of bananas**
- **Time of day**
- **Crowd levels**

#### Visualizations:
- **Bar charts** for daily consumption trends.
- **Line charts** for ripeness vs. consumption.
- **Heatmaps** to identify peak consumption times.

---

### Database Schema and Design {#database-schema}
#### Tables:
1. **restock_schedule**: Tracks restocking events and ripeness levels.
2. **consumption_log**: Records daily banana consumption (time, quantity, ripeness).
3. **crowd_levels**: Captures peak hours and crowd patterns.

#### Design Relationships:
- Link tables on key fields (e.g., `date` and `time`) to enable queries that combine datasets for deeper insights.

---

### Sample Period and Analytical Process {#sample-period}
#### Sample Period:
- **October**: The sample was limited to this month due to a lack of consistency during later weeks.

#### Analytical Process:
- **ERD Diagram**: [View Diagram](https://claude.site/artifacts/55ea6da5-dd04-4dda-8f25-7df26476609e)
- **Steps**:
  1. Calculate daily consumption averages.
  2. Explore correlations between ripeness and waste.
  3. Model predictive restocking strategies to optimize banana availability.





### Tab 1: Daily Trend



#### Chart A

```{r}

banana_plot <- ggplot(consumption_date, aes(x = as.Date(date_id), y = total_bananas)) +
  geom_line(color = "blue", size = 1) + 
  geom_point(color = "orange", size = 2, alpha = 0.6) +  
  facet_wrap(~is_weekend, labeller = as_labeller(c(`0` = "Non-Weekend", `1` = "Weekend"))) +
  scale_y_continuous(
    limits = c(0, 20),  
    breaks = seq(0, 20, by = 5)  
  ) +
  labs(title = "Daily Banana Consumption Over Time",
       x = "Date",
       y = "Total Bananas Consumed") +
  theme_dark() +
  theme(
    panel.grid.major = element_line(color = "lightgrey", size = 0.5),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "lightblue", color = "darkblue"),
    strip.text = element_text(color = "white", face = "bold"),
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.title.x = element_text(face = "bold", size = 12),
    axis.title.y = element_text(face = "bold", size = 12),
    axis.text.y = element_text(color = "white"),
     panel.background = element_rect(fill = "transparent", color = NA),  
    plot.background = element_rect(fill = "transparent", color = NA)   
  )

interactive_plot <- ggplotly(banana_plot)
interactive_plot


```

#### Chart B

```{r}
weeeknd_final <-  ggplot(consumption_date, aes(x = as.factor(is_weekend), y = total_bananas, fill = as.factor(is_weekend))) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_boxplot(width = 0.1) +
  labs(
    title = "Distribution of Banana Consumption: Weekdays vs. Weekends",
    x = "Is Weekend (0 = Weekday, 1 = Weekend)",
    y = "Total Bananas Consumed"
  ) +
  theme_minimal() +
  theme(
     legend.position = "none",
     panel.background = element_rect(fill = "transparent", color = NA),  
    plot.background = element_rect(fill = "transparent", color = NA),
    axis.title = element_text(color = "white"),
    axis.text = element_text(color = "#008080", size = 12)
  )

weekend_interactive <-  ggplotly(weeeknd_final)
renderPlotly({
  weekend_interactive
})

```
### Tab 2:Timed period

#### Chart A
```{r}
time_1 <- ggplot(by_time_long, aes(x = time_period, y = value, fill = metric)) +
  geom_col(position = "dodge") +
  facet_wrap(~metric, scales = "free") + 
  labs(
    title = "Total vs. Average Bananas Consumed per Time Period",
    x = "Time Period",
    y = "Value"
  ) +
  theme_dark() +
  theme(
    panel.grid.major = element_line(color = "lightgrey", size = 0.5),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "lightblue", color = "darkblue"),
    strip.text = element_text(color = "white", face = "bold"),
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.title.x = element_text(face = "bold", size = 12),
    axis.title.y = element_text(face = "bold", size = 12, color = "#008080"),
    axis.text.y = element_text(color = "black"),
    panel.background = element_rect(fill = "transparent", color = NA),
    plot.background = element_rect(fill = "transparent", color = NA),
    legend.background = element_rect(fill = "transparent", color = NA), 
    legend.text = element_text(color = "white", size = 12)
  )
vis_time_1 <- ggplotly(time_1)
vis_time_1
```

####ANOVA Table
```{r}
anova_results <- aov(bananas_consumed ~ time_period, data = Consumption_Log)
anova_df <- tidy(anova_results)
knitr::kable(anova_df)
```



####Tukey's post doc test to see what was different 
```{r}
tukey_results <- tidy(TukeyHSD(aov(bananas_consumed ~ time_period, data = Consumption_Log)))
tukey_plot <- ggplot(tukey_results, aes(x = contrast, y = estimate)) +
  geom_point(size = 3, color = "white") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "white") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "white") +
  coord_flip() +
  labs(
    title = "Tukey's HSD Test Results",
    x = "Comparison of time differnces ",
    y = "Mean Difference"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.background = element_rect(fill = "black", color = NA),
    plot.background = element_rect(fill = "black", color = NA),
    panel.grid.major = element_line(color = "gray"),
    panel.grid.minor = element_line(color = "darkgray"),
    axis.title = element_text(color = "white"),
    axis.text = element_text(color = "white"),
    plot.title = element_text(color = "white", hjust = 0.5)
  )

vistime_2 <- ggplotly(tukey_plot)
vistime_2
```

### Tab 3: Stuff that did not affect the relatiobship between CONSUMPTION

#### Ripeness optimization

```{r}
Consumption_Log$ripeness <- as.factor(Consumption_Log$ripeness)

ripeness_labels <- c(
  `1` = "Ripeness Level 1: Least Ripe",
  `2` = "Ripeness Level 2: Slightly Ripe",
  `3` = "Ripeness Level 3: Moderately Ripe",
  `4` = "Ripeness Level 4: Very Ripe",
  `5` = "Ripeness Level 5: Fully Ripe"
)


density_plot <- ggplot(Consumption_Log, aes(x = bananas_consumed, fill = ripeness)) +
  geom_density(alpha = 0.7, color = "blue") +
  facet_wrap(~ ripeness, scales = "free_y", labeller = as_labeller(ripeness_labels)) +
  theme_minimal() +
  labs(title = "Density of Banana Consumption by Ripeness",
       x = "Bananas Consumed",
       y = "Density") +
  scale_fill_viridis_d() +
  theme(panel.background = element_rect(fill = "transparent"),
        legend.position = "None",
        plot.background = element_rect(fill = "transparent", color = NA),
        axis.title = element_text(color = "white"),
        axis.ticks = element_line(color = "white"),
        axis.text = element_text(color = "white", size = 12, face = "bold"), 
        legend.text = element_text(color = "white", size = 14, face = "bold"), 
  
  )


density_plotly <- ggplotly(density_plot)
density_plotly
```
#### Kruskal Wallis Test results 
```{r}
kruskal_test <-  kruskal.test(bananas_consumed ~ ripeness, data = Consumption_Log)
kruskal_df <- tidy(kruskal_test)
knitr::kable(kruskal_df)
```


#### Bar graph
```{r}
Pie_chart_thrown <- bananas_thrown_away %>% 
  group_by(ripeness) %>%
  summarize(
    thrown_count = sum(thrown_away),
    total_count = n()
  ) %>%
  mutate(
    throwaway_rate = (thrown_count / total_count) * 100
  )

ripeness_labels <- c(
  `1` = "Ripeness Level 1: Least Ripe",
  `2` = "Ripeness Level 2: Slightly Ripe",
  `3` = "Ripeness Level 3: Moderately Ripe",
  `4` = "Ripeness Level 4: Very Ripe",
  `5` = "Ripeness Level 5: Fully Ripe"
)

faceted_data <-  Pie_chart_thrown %>%  
  pivot_longer(
    cols = c(thrown_count, throwaway_rate), 
    names_to = "metric", 
    values_to ="value"
  )
filtered_data <- faceted_data %>%
  filter(!(ripeness == "1" & value == 0))



bar <- ggplot(faceted_data, aes(x = "", y = value, fill = factor(ripeness))) +
  geom_bar(stat = "identity", position = "Dodge", width = 1) +
  facet_wrap(
    ~ metric, 
    scales = "free", 
    labeller = as_labeller(
      c(
        "thrown_count" = "Count of Bananas Thrown Away",
        "throwaway_rate" = "Percentage of Bananas Thrown Away"
      )
    )
  ) +
  labs(
    title = "Banana Throwaway Analysis by Ripeness",
    fill = "Ripeness Level", 
    x = "Ripeness level"
  ) +
  theme_void() +
  theme(
    legend.position = "right",
    plot.title = element_text(
      size = 15,       
      face = "bold",   
      hjust = .5, 
    )
  )

bar_plot <- ggplotly(bar)

bar_plot

```

#### Pie chart
```{r}
 ggplot(faceted_data, aes(x = "", y = value, fill = factor(ripeness))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y") +
  facet_wrap(
    ~ metric, 
    labeller = as_labeller(
      c(
        "thrown_count" = "Count of Bananas Thrown Away",
        "throwaway_rate" = "Percentage of Bananas Thrown Away"
      )
    )
  ) +
  labs(
    title = "Banana Throwaway Analysis by Ripeness",
    fill = "Ripeness Level"
  ) +
  theme_void() +
  theme(
    legend.position = "right",
    plot.title = element_text(
      size = 15,
      face = "bold",
      hjust = 0.5
    )
  )



```


### Tab 4: The dreaded ML part
*Welcome to the project dashboard for banana consumption optimization! This dashboard is designed to document the process, from data collection to insights, visualizations, and actionable outcomes.*


#### Table of Contents
- [Data Preparation](#data-preparation)
- [Cross-Validation](#cross-validation)
- [Models Used](#models-used)
- [Performance](#performance)

---

#### Data Preparation {data-preparation}
To begin, all categorical variables were one-hot encoded, converting them into numerical dummy variables. This ensured compatibility with machine learning models, particularly those requiring numerical inputs (e.g., Linear Regression and Support Vector Machines).  
- Encoding avoided the risk of the models inferring a false hierarchy from sequential integer labels.  
- Afterward, the dataset was split into **80% training** and **20% testing** to evaluate model performance on unseen data.  
- A dynamic formula was created to define the dependent variable (`bananas_consumed`) and all independent variables (the dummy variables). This approach standardized the modeling process across all machine learning models.

[Back to Top.](#The dreaded ML part)

---

#### Cross-Validation {cross-validation}
A **10-fold cross-validation** strategy was used to train and validate the models.  
- The data was divided into 10 subsets: during each iteration, 9 subsets were used for training, while the remaining subset was used for testing.  
- Each subset served as a testing set exactly once, ensuring comprehensive evaluation of model performance.  
- The average performance across all folds provided a reliable estimate of how each model would perform on unseen data.

[Back to Top.](#The dreaded ML part)

---

#### Models Used {models-used}
The following machine learning models were trained using the prepared dataset:  
1. **Linear Regression**: A baseline model to identify linear relationships between variables.  
2. **Multi-Linear Regression**: Extends Linear Regression by incorporating multiple predictors.  
3. **Random Forest**: An ensemble learning method that constructs multiple decision trees and averages their predictions.  
4. **Gradient Boosting**: Focuses on sequentially improving the model by minimizing residual errors in each iteration.  
5. **Neural Network**: A flexible model capable of capturing complex, nonlinear relationships in the data.

These models were trained using the dynamic formula created earlier, ensuring consistent feature selection across all implementations.

[Back to Top.](#The dreaded ML part)

---

#### Performance {performance}
To evaluate the models, metrics such as **Root Mean Squared Error (RMSE)** and **R-squared (R²)** were calculated:  
- **RMSE** quantified the prediction error, with lower values indicating better performance.  
- **R-squared (R²)** measured how well the models captured variance in banana consumption.  
- Cross-validation results were summarized using boxplots to visualize performance consistency.  

**Key Insights**:
- The **Random Forest** and **Gradient Boosting** models outperformed others in terms of accuracy and error minimization.  
- Neural networks provided flexibility but required careful tuning to optimize performance.  
- Predictions from the best-performing models closely matched actual consumption trends, as seen in scatterplots of predicted vs. actual values.

[Back to Top.](#ml-part)
